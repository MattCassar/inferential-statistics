{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two groups - drawing inferences \n",
    "\n",
    "### 1.01 Null Hypothesis Testing\n",
    "* Example of cat diets\n",
    "    * Why can't we just take $\n",
    "    \\mu_{\\text{raw}} - \\mu_{\\text{canned}}$ and determine whether there's an impact?\n",
    "        * This doesn't take into account any random fluctuations due to the distributions\n",
    "        * Precision due to sample size\n",
    "            * Could divide by variance to normalize\n",
    "        * Could use Bayesian approach to use distributions and compute probabilities\n",
    "    * Instead of arbitrarily guessing, formulate a null hypothesis and then show that this is unlikely to be true, at least below a certain probability (p-value)\n",
    "\n",
    "### 1.02 P-Values\n",
    "* Continuation of previous example using cat foot\n",
    "* Our null hypothesis is that there's no significant difference between the two diets\n",
    "    * $H_0: \\mu_1 - \\mu_2 = 0$\n",
    "* Also need to specify an alternative hypothesis\n",
    "    * We want to look at the area under the curve that represents a result that is at least as extreme as our null hypothesis\n",
    "    * Possible alternative hypotheses\n",
    "        * $H_a: \\mu_r - \\mu_c \\gt 0$ (unidirectional)\n",
    "            * take area to the right\n",
    "        * $H_a: \\mu_r - \\mu_c \\neq 0$ (bidrectional)\n",
    "            * take area to the right/left and then double it (p-value $\\cdot$ 2)\n",
    "        * $H_a: \\mu_r - \\mu_c \\lt 0$ (unidirectional)\n",
    "            * take area to the left\n",
    "    * Use $\\alpha$ to denote significance level (typical choice is $\\alpha = 0.05$)\n",
    "* Note: we never accept the null hypothesis, we just fail to reject it\n",
    "* Before statistical software it was really painful to compute p-values. People used to use tables (e.g. z-tables) to do the calculations. It's much easier now with stiatistical software.\n",
    "\n",
    "### 1.03 Confidence intervals and two-sided tests\n",
    "* *Confidence interval* (definition): if you were to repeatedly draw samples and calculate the statistic, then what is the range of values around the statistic if I want the population parameter to lie within the interval in 95% of my samples?\n",
    "* Two sided tests and confidence are closely related\n",
    "* Test decision is made whether boundary values are exceeded\n",
    "* With a test the interval is centered around the expected test statistic value under the null\n",
    "* With a confidence interval, the interval is centered around the sample statistic value\n",
    "* The margins around these intervals are the same\n",
    "    * In a test, the margin equals the critical test statistic value ($T_{\\alpha / 2}$) expressed in standard errors\n",
    "    * In an interval, the margin equals the critical test statistic value multiplied by the standard error ($T_{\\alpha / 2} \\cdot \\text{se}$) in original units of sample statistic\n",
    "\n",
    "### 1.04 Power\n",
    "* Power refers to probability of correctly rejecting the null hypothesis, the probability to detect a hypothesized effect if it truly exists in the population\n",
    "* Types of correct decisions/errors\n",
    "    * $H_0$ true, $H_0$ not rejected = true negative\n",
    "        * This probability of this is $1 - \\alpha$\n",
    "    * $H_0$ false, $H_0$ not rejected = false negative\n",
    "        * This probability of this is equal to confidence $\\alpha$\n",
    "        * Type I Error\n",
    "    * $H_0$ true, $H_0$ rejected = false positive\n",
    "        * This probability of this is $\\beta$\n",
    "        * Type II Error\n",
    "    * $H_0$ false, $H_0$ rejected = true positive\n",
    "        * The probability of this is called the *power* of the test and is $1 - \\beta$\n",
    "        * There are other ways of increasing power that don't require modifying $\\alpha$ or $\\beta$\n",
    "            * More observations/larger population increase power\n",
    "                * Standard error is $\\frac{s}{\\sqrt{n}} \\rightarrow$ smaller standard deviation, larger sample decreases the standard error\n",
    "            * How do we lower the standard deviation?\n",
    "                * Better instrument\n",
    "                * Control for variables that can increase variance\n",
    "                * Larger sample statistic $\\rightarrow$ the treatment in question had more of an effect and is easier to detect\n",
    "                * Use one-sided tests rather than two-sided tests because they are stronger in terms of assumptions\n",
    "                * Parameteric tests are stronger than non-parameteric tests\n",
    "* Estimate power\n",
    "    * Post-hoc (after we've collected samples)\n",
    "    * A priori (before we run the experiment)\n",
    "    * Standard effect size\n",
    "        * $T = \\frac{E - P_0}{\\text{se}}$\n",
    "    * Example\n",
    "        * Expected effect: medium\n",
    "        * $\\alpha = 0.05$\n",
    "        * $1 - \\beta = 0.80$\n",
    "        * Required n = ?\n",
    "            * 1544 (might be too expensive)\n",
    "            * Can increase alpha or use "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
